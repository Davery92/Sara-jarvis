# Sara's Advanced Episodic Memory System - Complete Guide

## ðŸ§  Overview

Sara's memory system is a sophisticated **episodic memory architecture** with emotional intelligence, importance scoring, and dynamic context windows. Built on PostgreSQL with pgvector extensions and powered by BGE-M3 embeddings, it creates a truly intelligent AI that remembers, learns, and grows from every interaction.

## ðŸŽ¯ Key Features

- **ðŸ§  Episodic Memory**: Every interaction stored as rich episodes with metadata
- **ðŸ’­ Emotional Intelligence**: Real-time emotional analysis and sentiment tracking
- **â­ Importance Scoring**: AI-calculated relevance scores (0-1 scale)
- **ðŸ” Dynamic Context Windows**: Intelligent memory retrieval based on query context
- **ðŸ˜´ Dream Processing**: Background consolidation and insight generation
- **ðŸ“Š Usage Analytics**: Access tracking and memory optimization
- **ðŸŽ¨ Multi-Source Support**: Chat, notes, documents, timers, and more

## ðŸ—ï¸ Architecture

### Core Components

```mermaid
graph TB
    A[User Interaction] --> B[IntelligentMemoryService]
    B --> C[EmotionalAnalyzer]
    B --> D[ImportanceCalculator]
    B --> E[TopicExtractor]
    B --> F[EmbeddingGenerator]
    
    C --> G[Episode Storage]
    D --> G
    E --> G
    F --> G
    
    H[Memory Search] --> I[ContextWindowManager]
    I --> J[Auto Window Selection]
    J --> K[Semantic Search]
    K --> L[PostgreSQL + pgvector]
    
    M[DreamingService] --> N[Background Analysis]
    N --> O[Pattern Detection]
    O --> P[Insight Generation]
    P --> Q[DreamInsight Storage]
    
    G --> L
    Q --> L
```

## ðŸ“Š Database Schema

### Episode Table (Core Memory Storage)
```sql
CREATE TABLE episode (
    id VARCHAR PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id VARCHAR,              -- Groups related interactions
    user_id VARCHAR NOT NULL,            -- User ownership
    role VARCHAR NOT NULL,               -- 'user' or 'assistant'
    content TEXT NOT NULL,               -- The actual interaction content
    
    -- Intelligence Metadata
    importance FLOAT DEFAULT 0.5,       -- AI-scored importance (0-1)
    emotional_tone TEXT,                 -- JSON: emotional analysis
    topics TEXT,                         -- JSON: extracted topics
    context_tags TEXT,                   -- JSON: contextual tags
    
    -- Memory Metadata  
    access_count INTEGER DEFAULT 0,     -- Usage tracking
    last_accessed TIMESTAMP,            -- Last retrieval time
    memory_type VARCHAR DEFAULT 'conversation',  -- conversation, note_creation, action
    source VARCHAR DEFAULT 'chat',      -- chat, note, document, timer, etc.
    
    -- Vector Search
    embedding VECTOR(1024),             -- BGE-M3 embeddings for similarity
    
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

### Context Window Table
```sql
CREATE TABLE context_window (
    id VARCHAR PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id VARCHAR NOT NULL,
    window_type VARCHAR NOT NULL,        -- temporal, topic, emotional, importance, hybrid
    parameters TEXT NOT NULL,            -- JSON configuration
    last_used TIMESTAMP,
    use_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Dream Insight Table
```sql
CREATE TABLE dream_insight (
    id VARCHAR PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id VARCHAR NOT NULL,
    insight_type VARCHAR NOT NULL,       -- pattern, connection, summary, etc.
    content TEXT NOT NULL,               -- Generated insight
    confidence FLOAT DEFAULT 0.5,       -- AI confidence (0-1)
    episode_ids TEXT,                    -- JSON: related episode IDs
    created_at TIMESTAMP DEFAULT NOW()
);
```

## ðŸ”§ Core Services

### IntelligentMemoryService

The central orchestrator that coordinates memory storage with advanced analysis:

```python
class IntelligentMemoryService:
    def __init__(self):
        self.window_manager = ContextWindowManager()
        self.emotional_analyzer = EmotionalAnalyzer()
    
    async def store_episode(
        self, 
        user_id: str, 
        role: str, 
        content: str, 
        conversation_id: str = None,
        source: str = "chat",
        memory_type: str = "conversation"
    ) -> Episode:
        """Store episode with full intelligence analysis"""
        
        # 1. Emotional Analysis
        emotional_analysis = await self.emotional_analyzer.analyze_emotional_state(content)
        
        # 2. Topic Extraction
        topics = await self._extract_topics(content)
        
        # 3. Importance Scoring
        importance = await self._calculate_importance(content, role, emotional_analysis)
        
        # 4. Embedding Generation
        embedding = await self._generate_embedding(content)
        
        # 5. Store with metadata
        episode = Episode(
            id=str(uuid.uuid4()),
            conversation_id=conversation_id,
            user_id=user_id,
            role=role,
            content=content,
            importance=importance,
            emotional_tone=json.dumps(emotional_analysis),
            topics=json.dumps(topics),
            source=source,
            memory_type=memory_type,
            embedding=embedding
        )
        
        return episode
```

#### Intelligence Components

**Emotional Analysis:**
```json
{
  "primary_emotion": "positive",
  "intensity": 0.7,
  "emotions": ["excitement", "confidence"],
  "sentiment_score": 0.85,
  "emotional_stability": "stable"
}
```

**Topic Extraction:**
```json
["productivity", "habit_tracking", "personal_development", "goal_setting"]
```

**Importance Calculation:**
- User questions: Higher importance
- Emotional intensity: Boosts importance
- Topic relevance: Contextual weighting
- Interaction length: Content depth factor

### EmotionalAnalyzer

Real-time emotional intelligence using fast LLM analysis:

```python
class EmotionalAnalyzer:
    def __init__(self):
        self.fast_model_url = OPENAI_BASE_URL
        self.fast_model = "gpt-oss:20b"  # Optimized for speed
        
    async def analyze_emotional_state(self, content: str) -> Dict[str, Any]:
        """Analyze emotional content using fast model"""
        
        prompt = f"""
        Analyze the emotional tone of this text and respond with JSON:
        
        Text: "{content}"
        
        Provide analysis in this exact format:
        {{
            "primary_emotion": "positive/negative/neutral",
            "intensity": 0.0-1.0,
            "emotions": ["specific", "emotions", "detected"],
            "sentiment_score": -1.0 to 1.0,
            "emotional_stability": "stable/volatile/mixed"
        }}
        """
        
        # Fast emotional analysis with structured output
        response = await self._call_fast_model(prompt)
        return self._parse_emotional_response(response)
```

### ContextWindowManager

Dynamic memory retrieval with intelligent window selection:

```python
class ContextWindowManager:
    async def auto_select_window(self, query: str, user_id: str) -> ContextWindowConfig:
        """Automatically select appropriate context window"""
        
        # Analyze query characteristics
        temporal_indicators = ["yesterday", "last week", "recently", "before"]
        emotional_indicators = ["felt", "happy", "sad", "excited", "worried"]
        topic_indicators = ["about", "regarding", "discussed", "talked"]
        
        if any(indicator in query.lower() for indicator in temporal_indicators):
            return TemporalWindowConfig(duration=timedelta(days=7))
            
        elif any(indicator in query.lower() for indicator in emotional_indicators):
            return EmotionalWindowConfig(emotional_states=["positive", "negative"])
            
        elif any(indicator in query.lower() for indicator in topic_indicators):
            topics = await self._extract_query_topics(query)
            return TopicWindowConfig(topics=topics)
            
        else:
            # Default hybrid window
            return HybridWindowConfig(
                duration=timedelta(days=30),
                min_importance=0.3,
                max_results=10
            )
```

#### Window Types

1. **Temporal Windows**: Time-based retrieval
   ```python
   TemporalWindowConfig(duration=timedelta(days=7), max_results=15)
   ```

2. **Topic Windows**: Subject-based filtering
   ```python
   TopicWindowConfig(topics=["productivity", "habits"], similarity_threshold=0.7)
   ```

3. **Emotional Windows**: Sentiment-based selection
   ```python
   EmotionalWindowConfig(emotional_states=["positive"], intensity_min=0.6)
   ```

4. **Importance Windows**: Significance-based filtering
   ```python
   ImportanceWindowConfig(min_importance=0.6, duration=timedelta(days=14))
   ```

5. **Hybrid Windows**: Multi-criteria selection
   ```python
   HybridWindowConfig(
       duration=timedelta(days=30),
       min_importance=0.3,
       topics=["work", "projects"],
       emotional_states=["positive", "neutral"]
   )
   ```

### DreamingService

Background processing for memory consolidation and insight generation:

```python
class DreamingService:
    def __init__(self):
        self.fast_model = "gpt-oss:20b"   # Quick analysis
        self.smart_model = "gpt-oss:120b" # Deep insights
        
    async def dream_cycle(self, user_id: str, min_episodes: int = 5):
        """Complete dreaming cycle for pattern detection"""
        
        if self.is_dreaming:
            return "Already dreaming..."
            
        self.is_dreaming = True
        
        try:
            # 1. Gather recent unanalyzed episodes
            episodes = await self._get_recent_episodes(user_id, min_episodes)
            
            if len(episodes) < min_episodes:
                return f"Need at least {min_episodes} episodes to dream"
            
            # 2. Pattern Detection
            patterns = await self._detect_patterns(episodes)
            
            # 3. Connection Analysis  
            connections = await self._analyze_connections(episodes)
            
            # 4. Generate Insights
            insights = await self._generate_insights(patterns, connections, episodes)
            
            # 5. Store Dream Insights
            for insight in insights:
                await self._store_dream_insight(user_id, insight)
                
            return f"ðŸŒ™ Dream cycle complete: {len(insights)} insights generated"
            
        finally:
            self.is_dreaming = False
```

#### Dream Insight Types

1. **Pattern Recognition**:
   ```json
   {
     "type": "pattern",
     "content": "User consistently asks about productivity in the mornings",
     "confidence": 0.85,
     "pattern_strength": "strong"
   }
   ```

2. **Connection Discovery**:
   ```json
   {
     "type": "connection", 
     "content": "Habit tracking discussions correlate with goal-setting conversations",
     "confidence": 0.92,
     "correlation_strength": 0.78
   }
   ```

3. **Behavioral Insights**:
   ```json
   {
     "type": "behavior",
     "content": "User engagement peaks during afternoon sessions",
     "confidence": 0.74,
     "trend_direction": "increasing"
   }
   ```

## ðŸ” Memory Search Architecture

### Intelligent Search Algorithm

```python
async def intelligent_memory_search(
    self, 
    user_id: str, 
    query: str, 
    auto_window: bool = True,
    limit: int = 10
) -> List[Dict]:
    """Multi-stage intelligent memory search"""
    
    # 1. Auto-select context window
    if auto_window:
        window_config = await self.window_manager.auto_select_window(query, user_id)
    else:
        window_config = HybridWindowConfig()  # Default
    
    # 2. Generate query embedding
    query_embedding = await self._generate_embedding(query)
    
    # 3. Execute contextual search with window
    episodes = await self._search_with_window(
        user_id=user_id,
        query_embedding=query_embedding,
        window_config=window_config,
        limit=limit
    )
    
    # 4. Update access tracking
    for episode in episodes:
        await self._update_access_tracking(episode['id'])
    
    # 5. Format results with metadata
    return self._format_search_results(episodes, window_config)
```

### Search Execution with Context Windows

**Temporal Window Search:**
```sql
SELECT e.*, 
       (1 - (e.embedding <=> %s)) as similarity
FROM episode e 
WHERE e.user_id = %s 
  AND e.created_at >= %s  -- Window temporal filter
  AND e.embedding IS NOT NULL
ORDER BY e.embedding <=> %s
LIMIT %s
```

**Topic Window Search:**
```sql
SELECT e.*, 
       (1 - (e.embedding <=> %s)) as similarity
FROM episode e 
WHERE e.user_id = %s 
  AND e.embedding IS NOT NULL
  AND (
    -- Topic filtering using JSON operators
    e.topics::jsonb ?| array['productivity', 'habits']
  )
ORDER BY e.embedding <=> %s  
LIMIT %s
```

**Emotional Window Search:**
```sql
SELECT e.*, 
       (1 - (e.embedding <=> %s)) as similarity
FROM episode e 
WHERE e.user_id = %s 
  AND e.embedding IS NOT NULL
  AND e.emotional_tone IS NOT NULL
  AND (
    -- Emotional filtering
    e.emotional_tone LIKE '%"primary_emotion": "positive"%'
    OR e.emotional_tone LIKE '%"primary_emotion": "neutral"%'
  )
ORDER BY e.embedding <=> %s
LIMIT %s
```

**Importance Window Search:**
```sql
SELECT e.*, 
       (1 - (e.embedding <=> %s)) as similarity
FROM episode e 
WHERE e.user_id = %s 
  AND e.importance >= %s  -- Importance threshold
  AND e.embedding IS NOT NULL
ORDER BY e.embedding <=> %s
LIMIT %s
```

## ðŸŽ¨ Advanced Features

### Multi-Source Memory Integration

Episodes can originate from multiple sources:

- **chat**: Direct conversations
- **note**: Note creation/editing  
- **document**: Document uploads/analysis
- **timer**: Timer activities
- **reminder**: Reminder interactions
- **habit**: Habit tracking events
- **calendar**: Calendar events

Each source provides contextual metadata for better retrieval.

### Memory Importance Calculation

```python
async def _calculate_importance(
    self, 
    content: str, 
    role: str, 
    emotional_analysis: Dict
) -> float:
    """Calculate episode importance (0-1 scale)"""
    
    base_importance = 0.5
    
    # Role weighting
    if role == "user":
        base_importance += 0.1  # User input slightly more important
    
    # Content length factor
    content_length = len(content)
    if content_length > 200:
        base_importance += 0.1
    elif content_length < 50:
        base_importance -= 0.1
        
    # Emotional intensity boost
    emotional_intensity = emotional_analysis.get('intensity', 0.5)
    base_importance += (emotional_intensity - 0.5) * 0.2
    
    # Question/command boost
    if content.strip().endswith('?') or content.lower().startswith(('how', 'what', 'when', 'why', 'where')):
        base_importance += 0.15
        
    # Technical content boost
    tech_keywords = ['code', 'api', 'database', 'function', 'error', 'bug']
    if any(keyword in content.lower() for keyword in tech_keywords):
        base_importance += 0.1
        
    return min(max(base_importance, 0.0), 1.0)  # Clamp to 0-1
```

### Access Pattern Analytics

```python
async def get_memory_analytics(self, user_id: str) -> Dict:
    """Comprehensive memory usage analytics"""
    
    db = SessionLocal()
    try:
        # Basic stats
        total_episodes = db.query(Episode).filter(Episode.user_id == user_id).count()
        
        # Most accessed memories
        popular_memories = db.query(Episode).filter(
            Episode.user_id == user_id
        ).order_by(Episode.access_count.desc()).limit(5).all()
        
        # Emotional distribution
        emotional_distribution = await self._analyze_emotional_patterns(user_id, db)
        
        # Topic trends
        topic_trends = await self._analyze_topic_trends(user_id, db)
        
        # Source breakdown
        source_breakdown = db.query(
            Episode.source, 
            func.count(Episode.id)
        ).filter(Episode.user_id == user_id).group_by(Episode.source).all()
        
        return {
            'total_episodes': total_episodes,
            'popular_memories': [{'content': m.content[:100], 'access_count': m.access_count} for m in popular_memories],
            'emotional_distribution': emotional_distribution,
            'topic_trends': topic_trends,
            'source_breakdown': dict(source_breakdown),
            'average_importance': await self._calculate_average_importance(user_id, db)
        }
        
    finally:
        db.close()
```

## ðŸ› ï¸ Tool Integration

### Memory Search Tool for LLM

```python
{
    "type": "function",
    "function": {
        "name": "search_memory",
        "description": "Search through Sara's enhanced episodic memory with emotional intelligence and context windows",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Search query to find relevant memories and past interactions"
                }
            },
            "required": ["query"]
        }
    }
}
```

### Search Response Format

```python
async def search_memory_tool(self, query: str, user_id: str) -> str:
    """Format search results for LLM consumption"""
    
    episodes = await intelligent_memory_service.intelligent_memory_search(
        user_id=user_id,
        query=query,
        auto_window=True
    )
    
    # Also include dream insights
    dream_insights = await self._search_dream_insights(query, user_id)
    
    # Format rich response
    response_parts = [
        f"ðŸ§  **Sara's Intelligent Memory Search: {len(episodes)} memories found for '{query}'**",
        "âœ¨ Using AI context window selection and emotional analysis"
    ]
    
    if dream_insights:
        response_parts.append(f"ðŸ’­ Found {len(dream_insights)} relevant insights from background analysis")
    
    response_parts.append("")
    
    # Format episode results
    for i, episode in enumerate(episodes[:6]):
        role_emoji = "ðŸ‘¤" if episode['role'] == "user" else "ðŸ¤–"
        
        # Parse metadata
        emotional_data = json.loads(episode['emotional_tone']) if episode['emotional_tone'] else {}
        topics_data = json.loads(episode['topics']) if episode['topics'] else []
        
        # Format with rich metadata
        response_parts.append(f"**{i+1}. {role_emoji} {episode['created_at'].strftime('%Y-%m-%d %H:%M')}**")
        
        if emotional_data.get('primary_emotion'):
            emotion_indicator = {"positive": "ðŸ˜Š", "negative": "ðŸ˜”", "neutral": "ðŸ˜"}.get(
                emotional_data['primary_emotion'], "ðŸ¤”"
            )
            response_parts.append(f"   {emotion_indicator} *{emotional_data['primary_emotion']} (intensity: {emotional_data.get('intensity', 0.5):.1f})*")
        
        if topics_data:
            response_parts.append(f"   ðŸ·ï¸ *Topics: {', '.join(topics_data[:3])}*")
            
        response_parts.append(f"   â­ *Importance: {episode['importance']:.2f}*")
        response_parts.append(f"   ðŸ“ *Source: {episode['source']}*")
        response_parts.append(f"   ðŸ’¬ {episode['content']}")
        response_parts.append("")
    
    return "\n".join(response_parts)
```

## ðŸ“ˆ Performance Optimization

### Database Indexing Strategy

```sql
-- Vector similarity search optimization
CREATE INDEX idx_episode_embedding 
ON episode USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- User-based partitioning
CREATE INDEX idx_episode_user_id ON episode (user_id);
CREATE INDEX idx_episode_user_created ON episode (user_id, created_at DESC);

-- Importance-based queries
CREATE INDEX idx_episode_importance ON episode (importance DESC) WHERE importance > 0.5;

-- Source and type filtering
CREATE INDEX idx_episode_source ON episode (source);
CREATE INDEX idx_episode_memory_type ON episode (memory_type);

-- Emotional analysis queries
CREATE INDEX idx_episode_emotional_tone ON episode USING gin ((emotional_tone::jsonb));

-- Topic-based searches
CREATE INDEX idx_episode_topics ON episode USING gin ((topics::jsonb));

-- Access pattern optimization
CREATE INDEX idx_episode_access_count ON episode (access_count DESC);
CREATE INDEX idx_episode_last_accessed ON episode (last_accessed DESC);
```

### Memory Management

```python
# Efficient embedding generation with caching
class EmbeddingCache:
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        
    async def get_embedding(self, text: str) -> List[float]:
        """Get embedding with LRU caching"""
        if text in self.cache:
            return self.cache[text]
            
        embedding = await self._generate_fresh_embedding(text)
        
        if len(self.cache) >= self.max_size:
            # Remove oldest entry
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            
        self.cache[text] = embedding
        return embedding

# Connection pooling optimization
DATABASE_POOL_SIZE = 20
DATABASE_MAX_OVERFLOW = 30
engine = create_engine(DATABASE_URL, pool_size=DATABASE_POOL_SIZE, max_overflow=DATABASE_MAX_OVERFLOW)

# Background processing queue
import asyncio
from asyncio import Queue

class MemoryProcessingQueue:
    def __init__(self):
        self.queue = Queue(maxsize=100)
        self.workers = []
        
    async def start_workers(self, num_workers: int = 3):
        """Start background workers for memory processing"""
        for i in range(num_workers):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)
            
    async def _worker(self, name: str):
        """Background worker for processing memory tasks"""
        while True:
            try:
                task = await self.queue.get()
                await self._process_memory_task(task)
                self.queue.task_done()
            except Exception as e:
                logger.error(f"Memory worker {name} error: {e}")
```

## ðŸ”§ Configuration & Setup

### Environment Variables

```bash
# Database Configuration
DATABASE_URL=postgresql+psycopg://sara:sara123@host:5432/sara_hub

# AI Models
OPENAI_BASE_URL=http://localhost:11434/v1
OPENAI_MODEL=gpt-oss:120b           # Main intelligence model
OPENAI_NOTIFICATION_MODEL=gpt-oss:20b  # Fast model for analysis

# Embedding Configuration  
EMBEDDING_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=bge-m3
EMBEDDING_DIM=1024

# Memory Settings
MEMORY_SEARCH_LIMIT=10
MEMORY_IMPORTANCE_THRESHOLD=0.3
DREAM_CYCLE_MIN_EPISODES=5
EMOTIONAL_ANALYSIS_ENABLED=true

# Performance Settings
DATABASE_POOL_SIZE=20
MEMORY_CACHE_SIZE=1000
BACKGROUND_WORKERS=3
```

### Docker Setup with pgvector

```yaml
# docker-compose.yml
services:
  db:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: sara_hub
      POSTGRES_USER: sara  
      POSTGRES_PASSWORD: sara123
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql

  sara-backend:
    build: ./backend
    environment:
      DATABASE_URL: postgresql+psycopg://sara:sara123@db:5432/sara_hub
      OPENAI_BASE_URL: http://ollama:11434/v1
      OPENAI_MODEL: gpt-oss:120b
    depends_on:
      - db
      - ollama

volumes:
  postgres_data:
```

### Database Initialization

```sql
-- init.sql
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Create optimized indexes after table creation
CREATE INDEX CONCURRENTLY idx_episode_embedding 
ON episode USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

CREATE INDEX CONCURRENTLY idx_episode_user_temporal 
ON episode (user_id, created_at DESC);

CREATE INDEX CONCURRENTLY idx_episode_importance_user 
ON episode (user_id, importance DESC) WHERE importance > 0.5;
```

## ðŸ“Š API Reference

### Core Memory Endpoints

#### Search Memory
```http
GET /memory/search?query=productivity&limit=10
Authorization: Bearer <token>

Response:
{
  "query": "productivity",
  "window_type": "hybrid",
  "total_results": 8,
  "results": [
    {
      "id": "uuid",
      "role": "user", 
      "content": "How can I improve my daily productivity?",
      "importance": 0.75,
      "emotional_tone": {
        "primary_emotion": "neutral",
        "intensity": 0.6
      },
      "topics": ["productivity", "improvement", "daily_habits"],
      "source": "chat",
      "similarity": 0.89,
      "created_at": "2024-01-15T10:30:00Z",
      "access_count": 3
    }
  ]
}
```

#### Get Memory Analytics
```http
GET /memory/analytics
Authorization: Bearer <token>

Response:
{
  "total_episodes": 1247,
  "average_importance": 0.52,
  "emotional_distribution": {
    "positive": 0.45,
    "neutral": 0.35, 
    "negative": 0.20
  },
  "source_breakdown": {
    "chat": 856,
    "note": 234,
    "document": 89,
    "timer": 45,
    "habit": 23
  },
  "popular_memories": [
    {
      "content": "Discussion about machine learning project architecture...",
      "access_count": 12,
      "importance": 0.89
    }
  ],
  "topic_trends": [
    {"topic": "productivity", "frequency": 89, "trend": "increasing"},
    {"topic": "programming", "frequency": 67, "trend": "stable"}
  ]
}
```

#### Trigger Dream Cycle
```http
POST /memory/dream
Authorization: Bearer <token>

Response:
{
  "status": "completed",
  "insights_generated": 5,
  "patterns_detected": 3,
  "connections_found": 8,
  "processing_time": "2.3s",
  "next_dream_recommended": "2024-01-20T10:00:00Z"
}
```

#### Get Dream Insights
```http  
GET /memory/insights?limit=5
Authorization: Bearer <token>

Response:
{
  "insights": [
    {
      "id": "uuid",
      "type": "pattern",
      "content": "User consistently asks about productivity optimization in morning sessions",
      "confidence": 0.87,
      "related_episodes": 12,
      "created_at": "2024-01-15T03:22:00Z"
    },
    {
      "id": "uuid", 
      "type": "connection",
      "content": "Strong correlation between habit tracking discussions and goal achievement",
      "confidence": 0.92,
      "correlation_strength": 0.84,
      "created_at": "2024-01-15T03:22:00Z"
    }
  ]
}
```

## ðŸ§ª Testing & Validation

### Memory Storage Tests

```python
async def test_episode_storage_with_intelligence():
    """Test complete intelligent episode storage"""
    
    # Test emotional analysis
    content = "I'm really excited about this new project!"
    episode = await intelligent_memory_service.store_episode(
        user_id="test_user",
        role="user", 
        content=content,
        source="chat"
    )
    
    # Verify emotional analysis
    emotional_data = json.loads(episode.emotional_tone)
    assert emotional_data['primary_emotion'] == 'positive'
    assert emotional_data['intensity'] > 0.6
    
    # Verify importance scoring
    assert episode.importance > 0.5  # Excited content should be important
    
    # Verify embedding generation
    assert episode.embedding is not None
    assert len(episode.embedding) == 1024  # BGE-M3 dimensions

async def test_context_window_selection():
    """Test automatic context window selection"""
    
    # Temporal query
    window = await context_manager.auto_select_window(
        "What did we discuss yesterday?", "test_user"
    )
    assert isinstance(window, TemporalWindowConfig)
    
    # Emotional query  
    window = await context_manager.auto_select_window(
        "When was I feeling happy?", "test_user"
    )
    assert isinstance(window, EmotionalWindowConfig)
    
    # Topic query
    window = await context_manager.auto_select_window(
        "Tell me about our productivity discussions", "test_user"
    )
    assert isinstance(window, TopicWindowConfig)

async def test_dream_insight_generation():
    """Test dream cycle insight generation"""
    
    # Store test episodes
    episodes = [
        ("I want to be more productive", "user"),
        ("Let's create a habit tracker", "assistant"), 
        ("How do I stay motivated?", "user"),
        ("Setting clear goals helps with motivation", "assistant"),
        ("I think habits and goals are connected", "user")
    ]
    
    for content, role in episodes:
        await intelligent_memory_service.store_episode(
            user_id="test_user",
            role=role,
            content=content
        )
    
    # Run dream cycle
    result = await dreaming_service.dream_cycle("test_user")
    
    # Verify insights generated
    assert "insights generated" in result
    
    # Check for pattern detection
    insights = await dreaming_service._get_dream_insights("test_user")
    assert len(insights) > 0
    
    # Verify insight quality
    productivity_insights = [i for i in insights if "productivity" in i.content.lower()]
    assert len(productivity_insights) > 0
```

### Performance Benchmarks

```python
async def benchmark_search_performance():
    """Benchmark memory search with different window types"""
    
    import time
    
    # Test data setup
    await setup_test_episodes(1000)  # 1000 test episodes
    
    # Benchmark semantic search
    start_time = time.time()
    results = await intelligent_memory_service.intelligent_memory_search(
        user_id="test_user",
        query="productivity habits",
        auto_window=True
    )
    semantic_time = time.time() - start_time
    
    # Benchmark different window types
    windows = [
        TemporalWindowConfig(duration=timedelta(days=7)),
        TopicWindowConfig(topics=["productivity"]),
        EmotionalWindowConfig(emotional_states=["positive"]),
        ImportanceWindowConfig(min_importance=0.6)
    ]
    
    window_times = {}
    for window in windows:
        start_time = time.time()
        await intelligent_memory_service._search_with_window(
            user_id="test_user",
            query_embedding=test_embedding,
            window_config=window
        )
        window_times[window.__class__.__name__] = time.time() - start_time
    
    # Performance assertions
    assert semantic_time < 1.0  # Should complete within 1 second
    assert all(t < 0.5 for t in window_times.values())  # Window searches should be fast
    assert len(results) <= 10  # Proper result limiting

async def benchmark_embedding_generation():
    """Benchmark embedding generation and caching"""
    
    # Test cold generation
    start_time = time.time()
    embedding1 = await embedding_service.generate_embedding("Test content for embedding")
    cold_time = time.time() - start_time
    
    # Test cached generation
    start_time = time.time() 
    embedding2 = await embedding_service.generate_embedding("Test content for embedding")
    cached_time = time.time() - start_time
    
    # Verify performance improvement
    assert cached_time < cold_time * 0.1  # Cache should be 10x faster
    assert embedding1 == embedding2  # Same content should give same embedding
```

## ðŸš¨ Troubleshooting

### Common Issues

#### Embedding Generation Failures
```python
async def diagnose_embedding_issues():
    """Diagnose embedding service problems"""
    
    try:
        # Test embedding service health
        health_check = await embedding_service.health_check()
        if not health_check:
            logger.error("âŒ Embedding service unreachable")
            return False
            
        # Test embedding generation
        test_embedding = await embedding_service.generate_embedding("test")
        if len(test_embedding) != EMBEDDING_DIM:
            logger.error(f"âŒ Wrong embedding dimensions: {len(test_embedding)} != {EMBEDDING_DIM}")
            return False
            
        logger.info("âœ… Embedding service healthy")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Embedding diagnosis failed: {e}")
        return False

async def fix_missing_embeddings():
    """Repair episodes with missing embeddings"""
    
    db = SessionLocal()
    try:
        # Find episodes without embeddings
        episodes_without_embeddings = db.query(Episode).filter(
            Episode.embedding.is_(None)
        ).limit(100).all()
        
        logger.info(f"Found {len(episodes_without_embeddings)} episodes without embeddings")
        
        # Generate missing embeddings
        for episode in episodes_without_embeddings:
            try:
                embedding = await embedding_service.generate_embedding(episode.content)
                episode.embedding = embedding
                db.commit()
                logger.info(f"âœ… Fixed embedding for episode {episode.id}")
                
            except Exception as e:
                logger.error(f"âŒ Failed to fix episode {episode.id}: {e}")
                
    finally:
        db.close()
```

#### Search Performance Issues
```python
async def diagnose_search_performance():
    """Diagnose and fix search performance issues"""
    
    db = SessionLocal()
    try:
        # Check index usage
        explain_query = """
        EXPLAIN (ANALYZE, BUFFERS) 
        SELECT * FROM episode 
        WHERE user_id = 'test' 
        ORDER BY embedding <=> '[0,0,0...]'::vector 
        LIMIT 10
        """
        
        result = db.execute(text(explain_query)).fetchall()
        
        # Look for index scans vs sequential scans
        plan_text = "\n".join([str(row[0]) for row in result])
        
        if "Index Scan" not in plan_text:
            logger.warning("âš ï¸ Query not using vector index - check index creation")
            
        if "Seq Scan" in plan_text:
            logger.warning("âš ï¸ Sequential scan detected - performance will be poor")
            
        # Check database statistics
        stats_query = """
        SELECT schemaname, tablename, n_tup_ins, n_tup_upd, n_tup_del, n_live_tup
        FROM pg_stat_user_tables 
        WHERE tablename = 'episode'
        """
        
        stats = db.execute(text(stats_query)).fetchone()
        if stats:
            logger.info(f"Episode table stats: {stats.n_live_tup} live tuples")
            
    finally:
        db.close()

async def optimize_search_performance():
    """Apply search performance optimizations"""
    
    db = SessionLocal()
    try:
        # Recreate vector index if needed
        db.execute(text("DROP INDEX IF EXISTS idx_episode_embedding"))
        db.execute(text("""
            CREATE INDEX idx_episode_embedding 
            ON episode USING ivfflat (embedding vector_cosine_ops) 
            WITH (lists = 100)
        """))
        
        # Update table statistics
        db.execute(text("ANALYZE episode"))
        
        # Vacuum to reclaim space
        db.execute(text("VACUUM ANALYZE episode"))
        
        db.commit()
        logger.info("âœ… Search performance optimization complete")
        
    finally:
        db.close()
```

#### Memory Leak Prevention
```python
async def monitor_memory_usage():
    """Monitor and prevent memory leaks"""
    
    import psutil
    import gc
    
    process = psutil.Process()
    
    # Memory usage monitoring
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # Run garbage collection
    gc.collect()
    
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    
    if memory_before - memory_after > 10:  # Significant memory freed
        logger.warning(f"Memory cleaned up: {memory_before - memory_after:.1f}MB freed")
    
    # Database connection monitoring
    db_connections = len(engine.pool.checkedout())
    if db_connections > DATABASE_POOL_SIZE * 0.8:
        logger.warning(f"High database connection usage: {db_connections}/{DATABASE_POOL_SIZE}")
    
    # Embedding cache monitoring
    cache_size = len(embedding_service.cache) if hasattr(embedding_service, 'cache') else 0
    logger.info(f"Memory usage: {memory_after:.1f}MB, DB connections: {db_connections}, Cache size: {cache_size}")
```

## ðŸ”® Future Enhancements

### Planned Intelligence Upgrades

1. **Advanced Pattern Recognition**
   - Multi-dimensional behavioral analysis
   - Predictive conversation modeling
   - Personalized response optimization

2. **Cross-User Learning** (Privacy-Preserving)
   - Anonymized pattern sharing
   - Collective intelligence insights
   - Federated learning integration

3. **Real-Time Adaptation**
   - Dynamic importance adjustment
   - Live emotional calibration
   - Adaptive context windows

4. **Memory Consolidation**
   - Automatic summary generation
   - Hierarchical memory structures
   - Long-term memory optimization

### Technical Roadmap

1. **Distributed Architecture**
   - Multi-node database sharding
   - Distributed embedding computation
   - Horizontal scaling support

2. **Advanced Caching**
   - Redis integration for hot memories
   - Predictive preloading
   - Memory locality optimization

3. **Real-Time Processing**
   - WebSocket-based live updates
   - Streaming memory analysis
   - Event-driven architecture

---

## ðŸ“š Summary

Sara's episodic memory system represents a sophisticated approach to AI memory that goes far beyond simple conversation storage. With emotional intelligence, importance scoring, dynamic context windows, and background dream processing, it creates a truly intelligent AI companion that learns, remembers, and grows with each interaction.

The system's multi-layered architecture ensures both performance and intelligence, making Sara capable of maintaining rich, contextual conversations that feel natural and deeply personal. This foundation enables Sara to be not just a chatbot, but a genuine AI assistant with human-like memory capabilities.